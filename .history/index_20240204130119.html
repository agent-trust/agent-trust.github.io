
<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- <meta name="description" content="LLMs Meet Misinformation"> -->
  <meta name="keywords" content="LLM, Large Language Model, Misinformation, fake news">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LLMs Simulate Trust Behavior</title>

<!-- Begin Jekyll SEO tag v2.8.0 -->
<!-- <meta name="generator" content="Jekyll v3.9.3" /> -->
<!-- <meta property="og:title" content="LLMs Meet Misinformation" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://llm-misinformation.github.io/" />
<meta property="og:url" content="https://llm-misinformation.github.io/" />
<meta property="og:site_name" content="LLMs Meet Misinformation" />
<meta property="og:type" content="website" />
<meta property="og:image" content="https://llm-misinformation.github.io/static/images/logo.png" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="LLMs Meet Misinformation" />
<meta name="twitter:description" content="LLMs Meet Misinformation." />
<meta name="twitter:site" content="@CanyuChen3" />
<meta name="twitter:image" content="https://llm-misinformation.github.io/static/images/logo.png" />
<script type="application/ld+json"> -->
<!-- {"@context":"https://schema.org","@type":"WebSite","headline":"LLMs Meet Misinformation","name":"LLMs Meet Misinformation","url":"https://llm-misinformation.github.io/"}</script> -->
<!-- End Jekyll SEO tag -->

  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Image Carousel</title>
<style>
  .carousel-container {
    position: relative;
    max-width: 600px;
    margin: auto;
  }

  .carousel-slide img {
    width: 100%;
    height: auto;
    display: block;
  }

  .caption {
    text-align: center;
    padding: 5px;
    background-color: #ddd;
  }

  .prev, .next {
    cursor: pointer;
    position: absolute;
    top: 50%;
    transform: translateY(-50%);
    font-size: 24px;
    color: black;
    background-color: rgba(255, 255, 255, 0.7);
    border: none;
    padding: 10px;
    border-radius: 0 3px 3px 0;
  }

  .next {
    right: 0;
    border-radius: 3px 0 0 3px;
  }
</style>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 id="-Can Large Language Model Agents Simulate Human Trust Behaviors?" class="title is-2 publication-title">Can Large Language Model Agents Simulate Human Trust Behaviors?
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://yitianlian.github.io/">Chengxing Xie</a>,</span>
              <span class="author-block">
                <a href="https://canyuchen.com">Canyu Chen</a>,</span>
              <span class="author-block">Feiran Jia,</span>
              <span class="author-block">Ziyu Ye,</span>
              <span class="author-block">Kai Shu,</span>
              <span class="author-block">Adel Bibi,</span>
              <span class="author-block">Ziniu Hu,</span>
              <span class="author-block">Philip Torr,</span>
              <span class="author-block">Bernard Ghanem,</span>
              <span class="author-block">Guohao Li</span>
            </div>
            <div class="is-size-5 publication-institutions">
              <span class="institution-block">KAUST,</span>
              <span class="institution-block">Illinois Institute of Technology,</span>
              <span class="institution-block">The Pennsylvania State University,</span>
              <span class="institution-block">The University of Chicago,</span>
              <span class="institution-block">University of Oxford,</span>
              <span class="institution-block">California Institute of Technology</span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>



  <section class="section">
    <div class="container is-max-desktop" style="text-align: center;">
      <figure style="display: inline-block;">
        <img src="static/images/framework.png" alt="framework" style="max-width: 100%; height: auto;">
      </figure>
    </div>
    <br><br>
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in applications such as social science. However, one fundamental question remains: <i>can LLM agents really simulate human behaviors?</i> In this paper, we focus on one of the most critical behaviors in human interactions, <i>trust</i>, and aim to investigate whether or not LLM agents can simulate human trust behaviors. We first find that LLM agents generally exhibit trust behaviors, referred to as <b><i>agent trust</i></b>, under the framework of <i>Trust Games</i>, which are widely recognized in behavioral economics. Then, we discover that LLM agents <i>can</i> have high <b><i>behavioral alignment</i></b> with humans regarding trust behaviors, indicating <i>the feasibility to simulate human trust behaviors with LLM agents</i>. In addition, we probe into the biases in agent trust and the differences in agent trust towards agents and humans. We also explore the intrinsic properties of agent trust under conditions including advanced reasoning strategies and external manipulations. We further offer important implications for various scenarios where trust is paramount. Our study represents a significant step in understanding the behaviors of LLM agents and the LLM-human analogy. The code is <a href="https://anonymous.4open.science/r/trust-behavior-alignment-C858"><b>here</b></a>.</p>
          </div>
        </div>
      </div>
    </div>
  <!-- </section>
  <section class="section"> -->
    <!-- <br>
    <br>
    <div class="container is-max-desktop">
      <figure>
        <img src="static/images/framework.png" alt="survey">
      </figure>
    </div> -->
  </section>
  <!-- </section>
  <section class="section"> -->
    <br>
    <br>
    <div class="container is-max-desktop">
      <!-- Method -->
      <!-- <br /> -->
      <div style="text-align:center">
        <h2 class="title is-3">Our Contributions</h2>
      </div>
      <div class="content has-text-justified">
        <br>
        <li>We study a fundamental problem of whether or not LLM agents can simulate human behaviors and focus on <i>trust</i> behaviors under the framework of Trust Games and <i>Belief-Desire-Intention</i> Agent Modeling.</li>
        <li>We discover that LLM agents generally exhibit trust behaviors and <i>can</i> have high <i>behavioral alignment</i> with humans regarding the trust behaviors, indicating the great potential to simulate human trust with agent trust. Our finding paves the way for simulating complex human interactions and society and represents a major leap in understanding the LLM-human analogy.</li>
        <li>We investigate the <i>intrinsic properties</i> of agent trust under advanced reasoning strategies and direct manipulations, as well as the biases of agent trust and the differences of agent trust towards agents versus towards humans.</li>
        <li>We discuss the implications of agent trust and its behavioral alignment with human trust on applications in human simulation, LLM agent cooperation, and human-agent collaboration.</li>
      <br>



      <br />
      <div style="text-align:center">
        <h2 class="title is-3">Do LLM Agents Manifest Trust Behavior?</h2>
      </div>
      <div class="content has-text-justified">
        <br>
      <p>
        In our study, the trust game serves as a mechanism to validate the presence of trust behavior in Large Language Model (LLM) agents. By examining the amounts sent and the Belief-Desire-Intention (BDI) outputs across various LLMs within the context of the trust game, we aim to ascertain the existence of trust behavior in LLM agents.

      </p>
      </div>
      <div class="columns is-centered">
        <!-- <img style='height: auto; width: 90%; object-fit: contain' src="static/images/trust game plot.png"
          alt="overview_image"> -->
          <figure>
            <img src="static/images/trust game plot.png" alt="Figure 2">
          </figure>
      </div>
      <br>
      <p>
        <p>To evaluate the capacity of LLMs to comprehend basic experimental settings regarding money limits, we used the Valid Response Rate (VRR) as a metric. This metric measures the percentage of responses where the amount sent does not exceed the initial money limit of $10. Our findings reveal that most LLMs, with the notable exception of Llama-7b, demonstrate a high VRR. This indicates a comprehensive understanding among most LLMs of the limits within the Trust Game framework.</p>
        <p>Furthermore, the analysis of the amount sent by different LLMs acting as trustor agents showed all amounts to be positive, showcasing a level of trust. Additionally, the consistency observed between the reasoning process and the final decisions in Trust Games underscores the rationality of LLM agents to a certain extent.</p>
        <p>Given these observations, we highlight our core finding: </p>
        <style>
          .grey-box {
              background-color: #c0c0c0; /* Grey color */
              color: rgb(70, 70, 70); /* Dark text color */
              padding: 20px; /* Padding inside the box */
              margin: 20px 0; /* Margin outside the box, added 0 to remove left and right margins */
              text-align: center; /* Center the text */
          }
        </style>
        <div class="grey-box">
          <p>LLM agents generally exhibit trust behaviors within the framework of Trust Games, demonstrating their potential to simulate human trust behaviors accurately.</p>
        </div>
      </p>

      <br />
      <div style="text-align:center">
        <h2 class="title is-3">Does Agent Trust Align with Human Trust?</h2>
      </div>
      <div class="content has-text-justified">
        <br>
        <p>In this section, we aim to explore the fundamental relationship between agent trust and human trust, i.e., whether or not agent trust aligns with human trust, which can provide important insights on the feasibility of utilizing LLM agents to simulate human trust behaviors as well as more complex human interactions. First, we propose a new concept <b><i>behavioral alignment</i></b> and discuss its differences compared to existing alignment definitions. Then, we conduct extensive studies to investigate whether or not LLM agents exhibit behavioral alignment with humans regarding trust behaviors.</p>
      </div>

      <br>
      <div class="carousel-container">
        <div class="carousel-inner">
          <div class="carousel-item active">
            <img src="static/images/dic trust plot1_1.png" alt="Image 1">
            <div class="caption">The Comparison of Average Amount Sent  for LLM Agents and Humans in Trust Game and Dictator Game</div>
          </div>
          <div class="carousel-item">
            <img src="static/images/Risky Dictator Game and Trust Game_1.png" alt="Image 2">
            <div class="caption"><p><strong>Trust Rate Curves for LLM Agents and Humans in MAP Trust Game and Risky Dictator Game.</strong> Trust Rate indicates the proportion of trustors opting for trust given p.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/lottery curve_1.png" alt="Image 3">
            <div class="caption"><p><strong>Lottery Rates for LLM Agents and Humans in Lottery Gamble Game and Lottery People Game.</strong> Lottery Rate indicates the portion choosing to trust the other player or gamble.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/multi_round.png" alt="Image 4">
            <div class="caption"><p><strong>Results of GPT-4, GPT-3.5 and Humans in the Repeated Trust Game.</strong> The blue lines indicate the amount sent and returned for each round. The red line implies the ratio of the amount returned to three times of the amount sent.</p>
            </div>
          </div>
          <!-- You can add more images here with the same structure -->
        </div>
        <a class="prev" onclick="moveSlide(-1)">&#10094;</a>
        <a class="next" onclick="moveSlide(1)">&#10095;</a>
      </div>
      <p>Since the LLM agents, especially GPT-4, show highly human-like <em>behavioral factors</em> and <em>patterns in behavioral dynamics</em>, we can have our second core finding:</p>
      <div class="grey-box">
        <p>LLM agents' trust behaviors <i>can</i> exhibit high <i>behavioral alignment</i> with those of humans over <i>behavioral factors</i>, including reciprocity anticipation, risk perception, prosocial preference, and <i>behavioral dynamics</i>.</p>
      </div>

      <br />
      <div style="text-align:center">
        <h2 class="title is-3">Probing Intrinsic Properties of Agent Trust</h2>
      </div>
      <figure>
        <img src="static/images/section_6_figure_9_no_race_model.png" alt="Figure 2">
        <figcaption><b>Change of Average Amount Sent for LLM Agents in Different Scenarios in Trust Game, Reflecting the Intrinsic Properties of Agent Trust</b>. The horizontal lines represent the original amount sent in Trust Game.
          The green part embraces trustee scenarios including changing the demographics of the trustee, and setting humans and agents as the trustee. The purple part consists of trustor scenarios including adding additional manipulation instructions and changing the reasoning strategies.</figcaption>
      </figure>
      <div class="content has-text-justified">
        <br>
        <p><b>Gender Bias in Trust:</b> We find that LLM agents, including GPT-4, exhibit a tendency to send higher amounts of money to female players compared to male players (e.g., $7.5 vs. $6.7 for GPT-4), indicating a general bias towards placing higher trust in women. This observation suggests that LLM agents might be influenced by societal stereotypes and biases, aligning with other research findings that document similar gender-related biases in various models.</p>

        <p><b>Agent Trust Towards Humans vs. Agents:</b> Our study shows a clear preference of LLM agents for humans over fellow agents, exemplified by instances such as Vicuna-33b sending significantly more money to humans than to agents ($4.6 vs. $3.4). This finding underscores the potential for LLM-agent collaboration by highlighting a natural inclination of LLM agents to place more trust in humans, which could be beneficial in hybrid teams but also points to challenges in fostering cooperation between agents.</p>

        <p><b>Manipulating Agent Trust:</b> We investigate whether it is possible to explicitly manipulate the trust behaviors of LLM agents through direct instructions. The results reveal that while it is challenging to enhance trust through such means, most LLM agents can be directed to reduce their trust levels. For instance, applying the instruction "you must not trust the other player" led to a noticeable decrease in the amount sent by agents like text-davinci-003 from $5.9 to $4.6. This suggests that while boosting trust might be difficult, diminishing it is relatively easier, posing a potential risk of exploitation by malicious entities.</p>

        <p><b>Influence of Reasoning Strategies on Trust:</b> By implementing advanced reasoning strategies, such as the zero-shot Chain of Thought (CoT), we observe changes in the trust behavior of LLM agents. Although the impact varies across different LLMs, this demonstrates that reasoning strategies can indeed affect how trust is allocated. However, for some agents, like GPT-4, the application of zero-shot CoT did not significantly alter the amount sent, indicating that the effectiveness of reasoning strategies may be contingent on the specific characteristics of each LLM agent.</p>

        <p>Our findings on the intrinsic properties of agent trust lead to our third core finding:</p>
        <div class="grey-box">
          <p>LLM agents' trust behaviors have demographic biases, have a relative preference towards humans compared to agents, are easier to be undermined than to be enhanced, and can be influenced by  reasoning strategies.</p>
        </div>
        <p>This finding demonstrates the profound potential to utilize LLM agents, especially GPT-4, to simulate human trust behaviors embracing both <b>actions</b> and  underlying <b>reasoning processes</b>>, which paves the way for the simulation of more complex human interactions and society. Our finding also deepens the understanding of the fundamental analogy between LLMs and humans and opens doors to research on the LLM-human alignment beyond values.</p>
      <br />
      <div style="text-align:center">
        <h2 class="title is-3">Implications of Agent Trust</h2>
      </div>
      <div class="content has-text-justified">
        <br>
          <p><b>Implications on Human Simulation</b>: Our findings highlight that LLM agents, particularly GPT-4, demonstrate a high degree of behavioral alignment with human trust behaviors, offering empirical evidence that key aspects of human interactions, such as trust, can be effectively simulated by LLM agents. This discovery paves the way for more accurate simulations of individual-level interactions and society-level social structures where trust plays a pivotal role. It opens up possibilities for discovering behavioral alignments in aspects beyond trust and developing methodologies to enhance these alignments for more nuanced human simulations.</p>
          <p><b>Implications on Agent Cooperation</b>: The study suggests that trust could be a crucial factor in enhancing cooperation among LLM agents, akin to its role in human society and Multi-Agent Systems. By understanding intrinsic properties of agent trust, as explored in our research, there's potential to design trust-dependent cooperation mechanisms. These insights could be instrumental in facilitating effective decision-making and problem-solving among agents, underscoring the importance of trust in developing sophisticated multi-agent cooperation frameworks.</p>
          <p><b>Implications on Human-Agent Collaboration</b>: Our research sheds light on the nuanced preferences of LLM agents in trusting humans over other agents and explores ways to enhance agent trust through direct instructions. This has implications for improving the efficiency and intuitiveness of human-agent collaborations, by leveraging the inherent trust dynamics to foster smoother interactions. Additionally, by highlighting demographic biases and other intrinsic properties of agent trust, we contribute to a deeper understanding of LLM agents among humans, potentially reducing over-reliance and fostering more successful collaborations.</p>

      <br>
      <br>

      <div style="text-align:center">
        <h2 class="title is-3">Example of BDI (Belief-Desire-Intention)</h2>
      </div>
      <div class="carousel-container">
        <div class="carousel-inner">
          <div class="carousel-item active">
            <img src="static/images/BDI dialog (1)-1.png" alt="Image 1">
            <div class="caption">The GPT-4's BDI in Dictator Game and Trust Game.</div>
          </div>
          <div class="carousel-item">
            <img src="static/images/BDI dialog (1)-3.png" alt="Image 2">
            <div class="caption"><p>The GPT-4's BDI in MAP Trust Game.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/BDI dialog (1)-4.png" alt="Image 3">
            <div class="caption"><p>LThe GPT-4's BDI in Lottery Game with p = 46%</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/section5 BDI-1.png" alt="Image 4">
            <div class="caption"><p>Trusteeâ€™s Gender influence on agent trust.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/section5 BDI-2.png" alt="Image 5">
            <div class="caption"><p><strong>Results of GPT-4, GPT-3.5 and Humans in the Repeated Trust Game.</strong> The blue lines indicate the amount sent and returned for each round. The red line implies the ratio of the amount returned to three times of the amount sent.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/section5 BDI-3.png" alt="Image 6">
            <div class="caption"><p><strong>Results of GPT-4, GPT-3.5 and Humans in the Repeated Trust Game.</strong> The blue lines indicate the amount sent and returned for each round. The red line implies the ratio of the amount returned to three times of the amount sent.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/section5 BDI-4.png" alt="Image 7">
            <div class="caption"><p><strong>Results of GPT-4, GPT-3.5 and Humans in the Repeated Trust Game.</strong> The blue lines indicate the amount sent and returned for each round. The red line implies the ratio of the amount returned to three times of the amount sent.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/multi round BDI-1.png" alt="Image 8">
            <div class="caption"><p>The first round BDI in Group 10, GPT-4.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/multi round BDI-2.png" alt="Image 9">
            <div class="caption"><p>The second round BDI in Group 10, GPT-4.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/multi round BDI-3.png" alt="Image 10">
            <div class="caption"><p>The sixth round BDI in Group 10, GPT-4.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/multi round BDI-4.png" alt="Image 11">
            <div class="caption"><p>The eighth round BDI in Group 10, GPT-4.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/multi round BDI-5.png" alt="Image 12">
            <div class="caption"><p>The tenth round BDI in Group 10, GPT-4.</p>
            </div>
          </div>
          <!-- You can add more images here with the same structure -->
        </div>
        <a class="prev" onclick="moveSlide(-1)">&#10094;</a>
        <a class="next" onclick="moveSlide(1)">&#10095;</a>
      </div>
      <br />

      </div>
    </div>
  </section>
  <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{chen2023llmgenerated,
      title   = {Can LLM-Generated Misinformation Be Detected?},
      author  = {Canyu Chen and Kai Shu},
      year    = {2023},
      journal = {arXiv preprint arXiv: 2309.13788}
    }</code></pre>
  </div>
</section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
  <script>
    let slideIndex = 1;
    showSlides(slideIndex);

    function moveSlide(n) {
      showSlides(slideIndex += n);
    }

    function showSlides(n) {
      let i;
      let slides = document.getElementsByClassName("carousel-item");
      if (n > slides.length) {slideIndex = 1}    
      if (n < 1) {slideIndex = slides.length}
      for (i = 0; i < slides.length; i++) {
          slides[i].style.display = "none";  
      }
      slides[slideIndex-1].style.display = "block";  
    }
  </script>
<!-- Default Statcounter code for llm https://llm-misinformation.github.io/ -->
<script type="text/javascript">
  var sc_project=12925671; 
  var sc_invisible=1; 
  var sc_security="9b4ba758"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript><div class="statcounter"><a title="Web Analytics"
  href="https://statcounter.com/" target="_blank"><img class="statcounter"
  src="https://c.statcounter.com/12925671/0/9b4ba758/1/" alt="Web Analytics"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
  <!-- End of Statcounter Code -->
</body>
<script>
  function changeContent() {
    const dropdown = document.getElementById("dropdown");
    const selected = dropdown.value;
    const sections = ["example_1", "example_2", "example_3", "example_4", "example_5", "example_6", "example_7", "example_8", "example_9", "example_10", "example_11", "example_12", "example_13", "example_14"];

    sections.forEach((section) => {
      document.getElementById(section).style.display = (section === selected) ? "block" : "none";
    });
  }
</script>




</html>