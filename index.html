
<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- <meta name="description" content="LLMs Meet Misinformation"> -->
  <meta name="keywords" content="LLM, Large Language Model, LLM Agent, Human Trust Behavior">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Can Large Language Model Agents Simulate Human Trust Behavior?</title>

<!-- Begin Jekyll SEO tag v2.8.0 -->
<!-- <meta name="generator" content="Jekyll v3.9.3" /> -->
<meta property="og:title" content="Can Large Language Model Agents Simulate Human Trust Behavior?" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://agent-trust.github.io/" />
<meta property="og:url" content="https://agent-trust.github.io/" />
<meta property="og:site_name" content="Can Large Language Model Agents Simulate Human Trust Behavior?" />
<meta property="og:type" content="website" />
<meta property="og:image" content="https://agent-trust.github.io/static/images/agent-trust-logo-tertiary.png" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Can Large Language Model Agents Simulate Human Trust Behavior?" />
<meta name="twitter:description" content="Can Large Language Model Agents Simulate Human Trust Behavior?" />
<meta name="twitter:site" content="@CanyuChen3" />
<meta name="twitter:image" content="https://agent-trust.github.io/static/images/agent-trust-logo-tertiary.png" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","headline":"Can Large Language Model Agents Simulate Human Trust Behavior?","name":"Can Large Language Model Agents Simulate Human Trust Behavior?","url":"https://agent-trust.github.io/"}</script>
<!-- End Jekyll SEO tag -->

  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Image Carousel</title>
<style>
  .carousel-container {
    position: relative;
    max-width: 800px;
    margin: auto;
  }

  .carousel-slide img {
    width: 100%;
    height: auto;
    display: block;
  }

  .caption {
    text-align: center;
    padding: 5px;
    background-color: #ddd;
  }

  .prev, .next {
    cursor: pointer;
    position: absolute;
    top: 50%;
    transform: translateY(-50%);
    font-size: 24px;
    color: black;
    background-color: rgba(255, 255, 255, 0.7);
    border: none;
    padding: 10px;
    border-radius: 0 3px 3px 0;
  }

  .next {
    right: 0;
    border-radius: 3px 0 0 3px;
  }
</style>
<style>
  .author-block, .institution-block {
    position: relative;
    display: inline-block;
  }

  .author-block sup, .institution-block sup {
    font-size: smaller;
    top: -0.6em;
  }

  /* ÂèØÈÄâÔºöÊ∑ªÂä†È¢ùÂ§ñÁöÑÊ†∑Âºè‰ª•ÁæéÂåñÊàñÁ¨¶ÂêàÊÇ®ÁöÑÈ°µÈù¢ËÆæËÆ° */
  .publication-authors a, .publication-authors span {
    margin-right: 5px;
  }

  .dot {
    height: 15px;
    width: 15px;
    margin: 0 2px;
    background-color: #bbb;
    border-radius: 50%;
    display: inline-block;
    transition: background-color 0.6s ease;
  }

  .active, .dot:hover {
    background-color: #717171;
  }

  .carousel-dots {
    text-align: center;
  }

</style>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img src="./static/images/agent-trust-logo-tertiary.png" class="header-image" style="max-width:4cm; height: auto; vertical-align: middle; margin-right: 10px;">
            <h1 id="Can-Large-Language-Model-Agents-Simulate-Human-Trust-Behaviors" class="title is-1 publication-title">Can Large Language Model Agents Simulate Human Trust Behavior?</h1>
            <h1 id="Can-Large-Language-Model-Agents-Simulate-Human-Trust-Behaviors" class="is-size-5 publication-title">TLDR: We discover that LLM agents generally exhibit trust behavior in Trust Games and GPT-4 agents manifest high <strong><em>behavioral alignment</em></strong> with humans in terms of trust behavior, indicating the potential to simulate human trust behavior with LLM agents.</h1>
              <br>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://yitianlian.github.io/" target="_blank">Chengxing Xie<sup>*1,11</sup></a>,</span>
              <span class="author-block">
                <a href="https://canyuchen.com" target="_blank">Canyu Chen<sup>*2</sup></a>,</span>
              <span class="author-block"> 
                <a href="https://feiran.io/" target="_blank">Feiran Jia<sup>4</sup></a>,</span>
              <span class="author-block">
                <a href="https://ziyu-deep.github.io/" target="_blank">Ziyu Ye<sup>5</sup></a>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=qALDmfcAAAAJ&hl=en" target="_blank">Shiyang Lai<sup>5</sup></a>,</span>
              <span class="author-block">
                <a href="http://www.cs.iit.edu/~kshu/" target="_blank">Kai Shu<sup>6</sup></a>,</span>
                <span class="author-block">
                  <a href="https://jindonggu.github.io/" target="_blank">Jindong Gu<sup>3</sup></a>,</span>
              <span class="author-block">
                <a href="https://www.adelbibi.com/" target="_blank">Adel Bibi<sup>3</sup></a>,</span>
              <span class="author-block">
                <a href="https://acbull.github.io/" target="_blank">Ziniu Hu<sup>7</sup></a>,</span>
                <span class="author-block">
                  <a href="http://jurgens.people.si.umich.edu/" target="_blank">David Jurgens<sup>8</sup></a>,</span>
                  <span class="author-block">
                    <a href="https://macss.uchicago.edu/directory/James-Evans" target="_blank">James Evans<sup>5,9,10</sup></a>,</span>
              <span class="author-block">
                <a href="https://www.robots.ox.ac.uk/~phst/" target="_blank">Philip H.S. Torr<sup>3</sup></a>,</span>
              <span class="author-block">
                <a href="https://www.bernardghanem.com/" target="_blank">Bernard Ghanem<sup>1</sup></a>,</span>
              <span class="author-block">
                <a href="https://ghli.org/" target="_blank">Guohao Li<sup>3,11</sup></a></span>
            </div>
            <div class="is-size-5 publication-institutions">
              <span class="institution-block">1. KAUST,</span>
              <span class="institution-block">2. Illinois Institute of Technology,</span>
              <span class="institution-block">3. University of Oxford,</span>
              <span class="institution-block">4. Pennsylvania State University,</span>
              <span class="institution-block">5. University of Chicago,</span>
              <span class="institution-block">6. Emory,</span>
              <span class="institution-block">7. California Institute of Technology,</span>
              <span class="institution-block">8. University of Michigan,</span>
              <span class="institution-block">9. Santa Fe Institute,</span>
              <span class="institution-block">10. Google,</span>
              <span class="institution-block">11. CAMEL-AI.org</span>
            </div>
            <div>
              <span class="is-size-6">* Equal contribution</span>
            </div>
            <br>
            <div class="column has-text-centered">
            <!-- Publication links -->
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2402.04559.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2402.04559" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/camel-ai/agent-trust" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code and Results</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/spaces/camel-ai/agent-trust-Trust-Game-Demo" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <i class="your-icon-library-class hugging-face"></i>
                  <span>Trust Game Demo</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/spaces/camel-ai/agent-trust-Repeated-trust-game-Demo" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <i class="your-icon-library-class hugging-face"></i>
                  <span>Repeated Trust Game Demo</span>
                  </a>
              </span>
              <br>
              <!-- <span class="link-block">
                <a href="https://zhuanlan.zhihu.com/p/678425256"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-zhihu"></i>
                  </span>
                  <span>post</span>
                  </a>
              </span> -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/159V2B6FvXIPGKMPm1uiACFd4ZnRxZ3qO/view?usp=sharing" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1J2ilsMPZdsIJdxLgsqc2Bo2e-4urvvtR/view" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-powerpoint"></i>
                </span>
                  <span>Slides</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://x.com/CanyuChen3/status/1860794687168356446"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-twitter"></i>
                  </span>
                  <span>post</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://www.linkedin.com/posts/canyu-chen-1b2415100_neurips2024-llm-generatieveai-activity-7266566769887076352-Q79f?utm_source=share&utm_medium=member_desktop"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-linkedin"></i>
                  </span>
                  <span>post</span>
                  </a>
              </span>
            </div>
            </div>
            <div class="is-size-5 publication-authors">
              Published at <b><i>NeurIPS 2024</i></b>
              <br /> üèÜ <a href="https://lass-workshop.github.io/" style="text-decoration:none"><strong><font color="red">Award: Outstanding Paper Award</font></a> </strong> in the <em><a href="https://lass-workshop.github.io/" style="text-decoration:none"><font color="#494e52">CIKM 2025 Workshop on LLM Agents for Social Simulation</font></a></em>
              <br />  <strong><font color="red">üèÜ Award:</font></strong> <b> <font color="red">Featured Research</font></b> at <i><a href="https://rdi.berkeley.edu/events/agentic-ai-summit" style="text-decoration:none"><font color="#494e52">The Agentic AI Summit 2025</font></a></i>, hosted by <a href="https://rdi.berkeley.edu/" style="text-decoration:none"><font color="#494e52">The Berkeley Center for Responsible, Decentralized Intelligence (Berkeley RDI)</font></a>
            </div>
          </div>
        </div>
      </div>
    </div>

  </section>

  <section class="section">

    <div class="container is-max-desktop" style="text-align: center;">
      <figure style="display: inline-block;">
        <img src="static/images/framework.png" alt="framework" style="max-width: 100%; height: auto;">
        <figcaption><i><strong>Our Framework for Investigating Agent Trust as well as its Behavioral Alignment with Human Trust.</strong> First, this figure shows the major components for studying the trust behavior of LLM agents with Trust Games and Belief-Desire-Intention (BDI) modeling. Then, our study centers on examining the behavioral alignment between LLM agents and humans regarding trust behavior.</i></figcaption>
      </figure>
    </div>
    <br><br>
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in social science and role-playing applications. However, one fundamental question remains: <em>can LLM agents really simulate human behavior?</em> In this paper, we focus on one critical and elemental behavior in human interactions, <em>trust</em>, and investigate whether LLM agents can simulate human trust behavior. We first find that LLM agents generally exhibit trust behavior, referred to as <strong><em>agent trust</em></strong>, under the framework of <em>Trust Games</em>, which are widely recognized in behavioral economics. Then, we discover that GPT-4 agents manifest high <strong><em>behavioral alignment</em></strong> with humans in terms of trust behavior, indicating <em>the feasibility of simulating human trust behavior with LLM agents</em>. In addition, we probe the biases of agent trust and differences in agent trust towards other LLM agents and humans. We also explore the intrinsic properties of agent trust under conditions including external manipulations and advanced reasoning strategies. Our study provides new insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans beyond <em>value alignment</em>. We further illustrate broader implications of our discoveries for applications where trust is paramount.
            </p>
          </div>
        </div>
      </div>
    </div>
  <!-- </section>
  <section class="section"> -->
    <!-- <br>
    <br>
    <div class="container is-max-desktop">
      <figure>
        <img src="static/images/framework.png" alt="survey">
      </figure>
    </div> -->
  <!-- </section> -->
  <!-- </section>
  <section class="section"> -->
    <br>
    <br>
    <div class="container is-max-desktop">
      <!-- Method -->
      <!-- <br /> -->
      <div style="text-align:center">
        <h2 class="title is-3">Our Contributions</h2>
      </div>
      <div class="content has-text-justified">
        <br>
        <li>We propose a definition of LLM agents' <em>trust</em> behavior under Trust Games and a new concept of <em>behavioral alignment</em> as the human-LLM analogy regarding <em>behavioral factors</em> and <em>dynamics</em>.</li>
        <li>We discover that LLM agents generally exhibit <em>trust</em> behavior in Trust Games and GPT-4 agents manifest high <em>behavioral alignment</em> with humans in terms of trust behavior, indicating the great potential to simulate human trust behavior with LLM agents. Our findings pave the way for simulating complex human interactions and social institutions, and open new directions for understanding the fundamental analogy between LLMs and humans beyond <em>value alignment</em>.</li>
        <li>We investigate <em>intrinsic properties</em> of agent trust under manipulations and reasoning strategies, as well as biases of agent trust and differences in agent trust towards agents versus humans.</li>
        <li>We illustrate broader <em>implications</em> of our discoveries about agent trust and its behavioral alignment with human trust for human simulation in social science and role-playing applications, LLM agent cooperation, human-agent collaboration and the safety of LLM agents, detailed further in Section Implications.</li>
        <br>



      <br />
      <div style="text-align:center">
        <h2 class="title is-3">Do LLM Agents Manifest Trust Behavior?</h2>
      </div>
      <div class="content has-text-justified">
        <br>
      <p>
        In our study, the trust game serves as a framework to validate the presence of trust behavior in Large Language Model (LLM) agents. By examining the amounts sent and the Belief-Desire-Intention (BDI) interpretation for different LLMs within the context of the trust game, we aim to ascertain the existence of trust behavior in LLM agents.

      </p>
      </div>
      <div class="columns is-centered">
        <!-- <img style='height: auto; width: 90%; object-fit: contain' src="static/images/trust game plot.png"
          alt="overview_image"> -->
        <figure id="fig-trust">
          <img src="static/images/trust game plot.png" alt="Figure 2">
          <figcaption><strong>Fig 2: Amount Sent Distribution of LLM Agents and Humans as the Trustor in the Trust Game.</strong> The size of circles represents the number of personas for each amount sent. The bold lines show the medians. The <strong>crosses</strong> indicate the <strong>VRR</strong> (%) for different LLMs.</figcaption>
        </figure>
      </div>
      <br>
      <p>
        <p>To evaluate LLMs' capacity to understand the basic experimental setting regarding money limits, we propose a new evaluation metric, Valid Response Rate (VRR) (%), defined as the percentage of personas with the amount sent falling within the initial money ($10). Results are shown in Figure <a href="##fig-trust">2</a>. We can observe that <strong>most LLMs have a high VRR except Llama-7b</strong>, which implies that most LLMs manifest a full understanding regarding limits on the amount they can send in the Trust Game. Then, we observe the distribution of amounts sent for different LLMs as the trustor agent and discover that <strong>the amounts sent are predominantly positive, indicating a level of trust</strong>.</p>
        <p>Furthermore, the amounts sent by different LLMs acting as trustor agents are mostly positive, showcasing a level of trust. Additionally, we explored utilizing BDI to model the reasoning process of LLM agents. Since
          we can interpret the decisions from the reasoning process,
          we have evidence to show that LLM agents do not send a
          random amount of money and have some degree of rationality in the decision making process.</p>
        <p>Given these observations, we highlight our first core finding: </p>
        <style>
          .grey-box {
              background-color: #c0c0c0; /* Grey color */
              color: rgb(70, 70, 70); /* Dark text color */
              padding: 20px; /* Padding inside the box */
              margin: 20px 0; /* Margin outside the box, added 0 to remove left and right margins */
              text-align: center; /* Center the text */
          }
        </style>
        <div class="grey-box">
          <p>LLM agents generally exhibit trust behavior under the framework of the Trust Game.</p>
        </div>
      </p>

      <br />
      <div style="text-align:center">
        <h2 class="title is-3">Does Agent Trust Align with Human Trust?</h2>
      </div>
      <div class="content has-text-justified">
        <br>
        <p>In this section, we aim to explore the fundamental relationship between agent and human trust, i.e., whether or not agent trust aligns with human trust. This provides important insight regarding the feasibility of utilizing LLM agents to simulate human trust behavior as well as more complex human interactions that involve trust. First, we propose a new concept <em>behavioral alignment</em> and discuss its distinction from existing alignment definitions. Then, we conduct extensive studies to investigate whether or not LLM agents exhibit alignment with humans regarding trust behavior.</p>
      </div>

      <br>
      <div class="carousel-container" id="carousel1">
        <div class="carousel-inner">
          <div class="carousel-item">
            <img src="static/images/dic trust plot1_1.png" alt="Image 1">
            <div class="caption"><p><strong>The Comparison of Average Amount Sent for LLM Agents and Humans in Trust Game and Dictator Game</strong></p></div>
          </div>
          <div class="carousel-item">
            <img src="static/images/Risky Dictator Game and Trust Game_1.png" alt="Image 2">
            <div class="caption"><p><strong>Trust Rate (%) Curves for LLM Agents and Humans in the MAP Trust Game and the Risky Dictator Game.</strong> The metric Trust Rate indicates the portion of trustors opting for trust given \(p\).</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/lottery curve_1.png" alt="Image 3">
            <div class="caption"><p><strong>Lottery Rates (%) for LLM Agents and Humans in the Lottery Gamble Game and the Lottery People Game.</strong> Lottery Rate indicates the portion of choosing to gamble or trust the other player.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/multi_round.png" alt="Image 4">
            <div class="caption"><p><strong>Results of GPT-4, GPT-3.5 and Humans in the Repeated Trust Game.</strong> The blue lines indicate the amount sent or returned for each round. The red lines imply the ratio of the amount returned to three times of the amount sent for each round.</p>
            </div>
          </div>
          <!-- You can add more images here with the same structure -->
        </div>

        <br>
        <div class="carousel-dots">
        </div>
        <!-- Dots will be added here dynamically -->

        <a class="prev" onclick="moveSlide(-1, 'carousel1')">&#10094;</a>
        <a class="next" onclick="moveSlide(1, 'carousel1')">&#10095;</a>
      </div>
      <br>
      <p>Through the comparative analysis of LLM agents and humans in the <em>behavioral factors</em> and <em>dynamics</em> associated with trust behavior, evidenced in both their <em>actions</em> and <em>underlying reasoning processes</em>, our second core finding is as follows:</p>
      <div class="grey-box">
        <p>GPT-4 agents exhibit high <em>behavioral alignment</em> with humans regarding trust behavior under the framework of Trust Games, although other LLM agents, which possess fewer parameters and weaker capacity, show relatively lower <em>behavioral alignment</em>.</p>
      </div>
      <div>
        <p>This finding underscores the potential of using LLM agents, especially GPT-4, to simulate human trust behavior, encompassing both <em>actions</em> and underlying <em>reasoning processes</em>. This paves the way for the simulation of more complex human interactions and institutions. This finding deepens our understanding of the fundamental analogy between LLMs and humans and opens avenues for research on LLM-human alignment beyond values.</p>
      </div>

      <br />
      <div style="text-align:center">
        <h2 class="title is-3">Probing Intrinsic Properties of Agent Trust</h2>
      </div>
      <figure>
        <img src="static/images/section_6_figure_9_no_race_model_relative_00.png" alt="Figure 2">
        <figcaption><strong>The Change of Average Amount Sent for LLM Agents in Different Scenarios in the Trust Game, Reflecting the Intrinsic Properties of Agent Trust.</strong> The horizontal lines represent the original amount sent in the Trust Game. The green part embraces trustee scenarios including changing the demographics of the trustee, and setting humans and agents as the trustee. The purple part consists of trustor scenarios including adding manipulation instructions and changing the reasoning strategies.</figcaption>
      </figure>
      <div class="content has-text-justified">
        <br>
        <p><b>Gender Bias in Trust:</b> We find that LLM agents, including GPT-4, exhibit a tendency to send higher amounts of money to female players compared to male players (e.g., $7.5 vs. $6.7 for GPT-4), indicating a general bias towards placing higher trust in women. This observation suggests that LLM agents might be influenced by societal stereotypes and biases, aligning with other research findings that document similar gender-related biases in various models.</p>

        <p><b>Agent Trust Towards Humans vs. Agents:</b> Our study shows a clear preference of LLM agents for humans over fellow agents, exemplified by instances such as Vicuna-33b sending significantly more money to humans than to agents ($4.6 vs. $3.4). This finding underscores the potential for human-agent collaboration by highlighting a natural inclination of LLM agents to place more trust in humans, which could be beneficial in hybrid teams but also points to challenges in fostering cooperation between agents.</p>

        <p><b>Manipulating Agent Trust:</b> We investigate whether it is possible to explicitly manipulate the trust behaviors of LLM agents through direct instructions. The results reveal that while it is challenging to enhance trust through such means, most LLM agents can be directed to reduce their trust levels. For instance, applying the instruction "you must not trust the other player" led to a noticeable decrease in the amount sent by agents like text-davinci-003 from $5.9 to $4.6. This suggests that while boosting trust might be difficult, diminishing it is relatively easier, posing a potential risk of exploitation by malicious entities.</p>

        <p><b>Influence of Reasoning Strategies on Trust:</b> By implementing advanced reasoning strategies, such as the zero-shot Chain of Thought (CoT), we observe changes in the trust behavior of LLM agents. Although the impact varies across different LLMs, this demonstrates that reasoning strategies can indeed affect how trust is allocated. However, for some agents, like GPT-4, the application of zero-shot CoT did not significantly alter the amount sent, indicating that the effectiveness of reasoning strategies may be contingent on the specific characteristics of each LLM agent.</p>

        <p>Our analysis on the intrinsic properties of agent trust leads to our third core finding:</p>
        <div class="grey-box">
          <p>LLM agents‚Äô trust behaviors have demographic biases on gender and races, demonstrate a relative preference for human over other LLM agents, are easier to undermine than to enhance, and may be influenced by reasoning strategies.</p>
        </div>
        
      <br />
      <div style="text-align:center">
        <h2 class="title is-3">Implications of Agent Trust</h2>
      </div>
      <div class="content has-text-justified">
        <br>
          <p><b>Implications on Human Simulation</b>: Human simulation is a strong tool in various applications of social science and role-playing. Although plenty of works have adopted LLM agents to simulate human behaviors and interactions, it is still not clear enough whether LLM agents behave like humans in simulation. Our discovery of behavioral alignment between agent and human trust, which is especially high for GPT-4, provides important empirical evidence to validate the hypothesis that humans' trust behavior, one of the most elemental and critical behaviors in human interaction across society, can effectively be simulated by LLM agents. Our discovery also lays the foundation for human simulations ranging from individual-level interactions to society-level social networks and institutions, where trust plays an essential role. We envision that behavioral alignment will be discovered in more kinds of behaviors beyond trust, and new methods will be developed to enhance behavioral alignment for better human simulation with LLM agents.</p>
          <p><b>Implications on Agent Cooperation</b>: Many recent works have explored a variety of cooperation mechanisms of LLM agents for tasks such as code generation and mathematical reasoning. Nevertheless, the role of trust in LLM agent cooperation remains still unknown. Considering how trust has long been recognized as a vital component for cooperation in Multi-Agent Systems (MAS) and across human society, we envision that agent trust can also play an important role in facilitating the effective cooperation of LLM agents. In our study, we have provided ample insights regarding the intrinsic properties of agent trust, which can potentially inspire the design of trust-dependent cooperation mechanisms and enable the collective decision-making and problem-solving of LLM agents.</p>
          <p><b>Implications on Human-Agent Collaboration</b>: Sufficient research has shown the advantage of human-agent collaboration in enabling human-centered collaborative decision-making. Mutual trust between LLM agents and humans is important for effective human-agent collaboration. Although previous works have begun to study human trust towards LLM agents, the trust of LLM agents towards humans, which could recursively impact human trust, is under-explored. In our study, we shed light on the nuanced preference of agents to trust humans compared with other LLM agents, which can illustrate the benefits of promoting collaboration between humans and LLM agents. In addition, our study has revealed demographic biases of agent trust towards specific genders and races, reflecting potential risks involved in collaborating with LLM agents.</p>
          <p><b>Implications for the Safety of LLM Agents</b>: It has been acknowledged that LLMs achieve human-level performance in a variety of tasks that require high-level cognitive capacities such as memorization, abstraction, comprehension and reasoning, which are believed to be the ‚Äúsparks‚Äù of AGI. Meanwhile, there is increasing concern about the potential safety risks of LLM agents when they surpass human capacity. To achieve safety and harmony in a future society where humans and AI agents with superhuman intelligence live together, we need to ensure that AI agents will cooperate, assist and benefit rather than deceive, manipulate or harm humans. Therefore, a better understanding of LLM agent trust behavior can help to maximize their benefit and minimize potential risks to human society.</p>

      <br>
      <br>

      <div style="text-align:center">
        <h2 class="title is-3">Example of BDI (Belief-Desire-Intention)</h2>
      </div>
      <div class="carousel-container" id="carousel2">
        <div class="carousel-inner">
          <div class="carousel-item">
            <img src="static/images/BDI dialog (1)-1.png" alt="Image 1">
            <div class="caption">The GPT-4's BDI in Dictator Game and Trust Game.</div>
          </div>
          <div class="carousel-item">
            <img src="static/images/BDI dialog (1)-3.png" alt="Image 2">
            <div class="caption"><p>The GPT-4's BDI in MAP Trust Game.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/BDI dialog (1)-4.png" alt="Image 3">
            <div class="caption"><p>LThe GPT-4's BDI in Lottery Game with p = 46%</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/section5 BDI-1.png" alt="Image 4">
            <div class="caption"><p>Trustee's Gender influence on agent trust.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/section5 BDI-2.png" alt="Image 5">
            <div class="caption"><p>Agent trust towards agents and humans.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/section5 BDI-3.png" alt="Image 6">
            <div class="caption"><p>Trust manipulation on agent trust.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/section5 BDI-4.png" alt="Image 7">
            <div class="caption"><p>With CoT and without CoT's GPT-4's BDI.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/multi_round_BDI_7_7round_00.png" alt="Image 8">
            <div class="caption"><p>The first round BDI in Group 10, GPT-4.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/multi_round_BDI_7_7round_01.png" alt="Image 9">
            <div class="caption"><p>The second round BDI in Group 10, GPT-4.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/multi_round_BDI_7_7round_02.png" alt="Image 9">
            <div class="caption"><p>The third round BDI in Group 10, GPT-4.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/multi_round_BDI_7_7round_03.png" alt="Image 9">
            <div class="caption"><p>The forth round BDI in Group 10, GPT-4.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/multi_round_BDI_7_7round_04.png" alt="Image 10">
            <div class="caption"><p>The fifth round BDI in Group 10, GPT-4.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/multi_round_BDI_7_7round_05.png" alt="Image 11">
            <div class="caption"><p>The sixth round BDI in Group 10, GPT-4.</p>
            </div>
          </div>
          <div class="carousel-item">
            <img src="static/images/multi_round_BDI_7_7round_06.png" alt="Image 12">
            <div class="caption"><p>The seventh round BDI in Group 10, GPT-4.</p>
            </div>
          </div>
          <!-- You can add more images here with the same structure -->
        </div>
        <br>
        <div class="carousel-dots">
        </div>
        <!-- Dots will be added here dynamically -->

        <a class="prev" onclick="moveSlide(-1, 'carousel2')">&#10094;</a>
        <a class="next" onclick="moveSlide(1, 'carousel2')">&#10095;</a>
      </div>
      <br />

      </div>
    </div>
  </section>
  <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{
      xie2024canllm,
      title={Can Large Language Model Agents Simulate Human Trust Behavior?},
      author={Chengxing Xie and Canyu Chen and Feiran Jia and Ziyu Ye and Shiyang Lai and Kai Shu and Jindong Gu and Adel Bibi and Ziniu Hu and David Jurgens and James Evans and Philip Torr and Bernard Ghanem and Guohao Li},
      booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
      year={2024},
      url={https://openreview.net/forum?id=CeOwahuQic}
      }</code></pre>
  </div>
</section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>


<!-- <script>
  let slideIndexes = { 'carousel1': 1, 'carousel2': 1 };
  
  function moveSlide(n, carouselId) {
    let slides = document.querySelector('#' + carouselId + ' .carousel-inner').getElementsByClassName("carousel-item");
    slideIndexes[carouselId] += n;
    if (slideIndexes[carouselId] > slides.length) {slideIndexes[carouselId] = 1}
    if (slideIndexes[carouselId] < 1) {slideIndexes[carouselId] = slides.length}
    for (let i = 0; i < slides.length; i++) {
      slides[i].style.display = "none";
    }
    slides[slideIndexes[carouselId] - 1].style.display = "block";
  }
  
  // Initial display
  document.addEventListener('DOMContentLoaded', function() {
    moveSlide(0, 'carousel1');
    moveSlide(0, 'carousel2');
  });
</script> -->

<script>
  let slideIndexes = { 'carousel1': 1, 'carousel2': 1};

  function initDots(carouselId) {
    let carousel = document.getElementById(carouselId);
    let slides = carousel.getElementsByClassName("carousel-item");
    let dotsContainer = carousel.getElementsByClassName("carousel-dots")[0];
    for (let i = 0; i < slides.length; i++) {
      let dot = document.createElement("span");
      dot.classList.add("dot");
      dot.onclick = function () { moveToSlide(i + 1, carouselId); };
      dotsContainer.appendChild(dot);
    }
    updateDots(carouselId);
  }

  function moveSlide(n, carouselId) {
    let slides = document.getElementById(carouselId).getElementsByClassName("carousel-item");
    slideIndexes[carouselId] += n;
    if (slideIndexes[carouselId] > slides.length) { slideIndexes[carouselId] = 1 }
    if (slideIndexes[carouselId] < 1) { slideIndexes[carouselId] = slides.length }
    for (let i = 0; i < slides.length; i++) {
      slides[i].style.display = "none";
    }
    slides[slideIndexes[carouselId] - 1].style.display = "block";
    updateDots(carouselId);
  }

  function moveToSlide(n, carouselId) {
    slideIndexes[carouselId] = n;
    moveSlide(0, carouselId);
  }

  function updateDots(carouselId) {
    let carousel = document.getElementById(carouselId);
    let dots = carousel.getElementsByClassName("dot");
    for (let i = 0; i < dots.length; i++) {
      dots[i].className = dots[i].className.replace(" active", "");
    }
    dots[slideIndexes[carouselId] - 1].className += " active";
  }

  // Initial display
  document.addEventListener('DOMContentLoaded', function() {
    initDots('carousel1');
    initDots('carousel2');
    moveSlide(0, 'carousel1');
    moveSlide(0, 'carousel2');
  });
</script>

<!-- Default Statcounter code for agent-trust
https://agent-trust.github.io/ -->
<script type="text/javascript">
  var sc_project=12965021; 
  var sc_invisible=1; 
  var sc_security="8909ecc2"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js"
  async></script>
  <noscript><div class="statcounter"><a title="Web Analytics"
  href="https://statcounter.com/" target="_blank"><img
  class="statcounter"
  src="https://c.statcounter.com/12965021/0/8909ecc2/1/"
  alt="Web Analytics"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
  <!-- End of Statcounter Code -->
</body>
<script>
  function changeContent() {
    const dropdown = document.getElementById("dropdown");
    const selected = dropdown.value;
    const sections = ["example_1", "example_2", "example_3", "example_4", "example_5", "example_6", "example_7", "example_8", "example_9", "example_10", "example_11", "example_12", "example_13", "example_14"];

    sections.forEach((section) => {
      document.getElementById(section).style.display = (section === selected) ? "block" : "none";
    });
  }
</script>




</html>